# Packages
import os, time, json, requests
import pandas as pd
from datetime import datetime
from typing import Dict, List, Union

# Getting Token
def be_login(username: str, password: str) -> str:
    url = f"{BASE_URL_BE}/v1/auth/login"
    r = requests.post(
        url,
        headers={"Content-Type": "application/json", "Accept": "application/json"},
        json={"username": username, "password": password},
        timeout=30,
    )
    r.raise_for_status()
    tok = r.json().get("access_token")
    if not tok:
        raise RuntimeError("No 'access_token' in login response.")
    return tok


# ---------- API call ----------
def create_audit_report(
    token: str,
    client_id: str,
    controlset_id: str,
    *,
    compliance_name: str = "Batch Report",
    description: str = "v1",
    evidence_files: List[dict] | None = None,
    compliance_prompt: str | None = None,
    org_id: str | None = None,
    job_additional_information: dict | None = None,
) -> dict:
    url = f"{BASE_URL_BE}/v1/ecfr/add_compliance"
    payload = {
        "client_id": client_id,
        "controlset_id": controlset_id,
        "compliance_name": compliance_name,
        "description": description,
    }
    if evidence_files:
        payload["evidence_files"] = evidence_files
    if compliance_prompt is not None:
        payload["compliance_prompt"] = compliance_prompt
    if org_id:
        payload["org_id"] = org_id
    if job_additional_information:
        payload["job_additional_information"] = job_additional_information

    r = requests.post(
        url,
        headers={"Authorization": f"Bearer {token}", "Content-Type": "application/json", "Accept": "application/json"},
        json=payload,
        timeout=60,
    )
    r.raise_for_status()
    return r.json()

# ---------- Evidence helpers ----------
def _to_evidence_item(path_or_item: Union[str, dict]) -> dict:
    """Convert 'C:/folder/file.pdf' or 's3://bucket/folder/file.pdf' to {'source': 'file.pdf', 'directory': 'C:/folder'}."""
    if isinstance(path_or_item, dict):
        if {"source", "directory"} <= set(path_or_item.keys()):
            return {"source": str(path_or_item["source"]), "directory": str(path_or_item["directory"])}
        raise ValueError(f"Evidence dict missing keys: {path_or_item}")
    p = str(path_or_item).rstrip("/")
    return {"source": os.path.basename(p), "directory": os.path.dirname(p)}

def build_evidence_files_for_client(
    client_id: str, evidence_map: Dict[str, Union[str, List[Union[str, dict]]]]
) -> List[dict]:
    if client_id not in evidence_map:
        return []
    val = evidence_map[client_id]
    if isinstance(val, (list, tuple)):
        return [_to_evidence_item(x) for x in val]
    return [_to_evidence_item(val)]

# ---------- Batch runner (requires companies_requiring_audit) ----------
def run_batch_audits_with_map(
    token: str,
    evidence_map: Dict[str, Union[str, List[Union[str, dict]]]],
    companies_requiring_audit: pd.DataFrame,
    *,
    iq_id_col: str = "IQ ID",
    controlset_col: str = "Control_Set_ID",
    compliance_name_prefix: str = "Audit",
    description: str = "v1",
    compliance_prompt: str | None = "You are a deterministic compliance evaluator. Cite exact evidence.",
    org_id: str | None = None,
    per_request_sleep_sec: float = 0.5,  # gentle pacing between requests
) -> pd.DataFrame:
    """
    For every client_id in evidence_map, look up CONTROL_SET_ID from companies_requiring_audit and create the audit.
    Saves JSONL and CSV; returns a summary DataFrame.
    """
    # 1) Build a lookup dict {client_id(str) -> controlset_id(str)}
    if iq_id_col not in companies_requiring_audit.columns or controlset_col not in companies_requiring_audit.columns:
        raise ValueError(f"companies_requiring_audit must include columns '{iq_id_col}' and '{controlset_col}'.")

    df_map = companies_requiring_audit[[iq_id_col, controlset_col]].dropna()
    # coerce to string, strip whitespace
    df_map[iq_id_col] = df_map[iq_id_col].astype(str).str.strip()
    df_map[controlset_col] = df_map[controlset_col].astype(str).str.strip()

    # If duplicates exist, keep the first and warn
    if df_map[iq_id_col].duplicated().any():
        dups = df_map[df_map[iq_id_col].duplicated()][iq_id_col].unique().tolist()
        print(f"Warning: duplicate '{iq_id_col}' values found; using the first occurrence for: {dups[:10]}{'...' if len(dups)>10 else ''}")
        df_map = df_map.drop_duplicates(subset=[iq_id_col], keep="first")

    CONTROLSET_MAP = dict(zip(df_map[iq_id_col], df_map[controlset_col]))

    results = []
    for idx, client_id in enumerate(evidence_map.keys(), start=1):
        client_id_str = str(client_id).strip()
        controlset_id = CONTROLSET_MAP.get(client_id_str)

        if not controlset_id:
            results.append({
                "client_id": client_id_str,
                "controlset_id": None,
                "job_status": None,
                "status": None,
                "compliance_name": None,
                "created_by": None,
                "org_id": org_id,
                "response": None,
                "error": f"No CONTROL_SET_ID found for client_id='{client_id_str}'"
            })
            continue

        try:
            evidence_files = build_evidence_files_for_client(client_id_str, evidence_map)
            if not evidence_files:
                results.append({
                    "client_id": client_id_str,
                    "controlset_id": controlset_id,
                    "job_status": None,
                    "status": None,
                    "compliance_name": None,
                    "created_by": None,
                    "org_id": org_id,
                    "response": None,
                    "error": "No evidence files found for this client in EVIDENCE_MAP"
                })
                continue

            resp = create_audit_report(
                token,
                client_id_str,
                controlset_id,
                compliance_name=f"{compliance_name_prefix} - {client_id_str}",
                description=description,
                evidence_files=evidence_files,
                compliance_prompt=compliance_prompt,
                org_id=org_id,
            )
            row = {
                "client_id": client_id_str,
                "controlset_id": controlset_id,
                "job_status": resp.get("job_status"),
                "status": resp.get("status"),
                "compliance_name": resp.get("compliance_name"),
                "created_by": resp.get("created_by"),
                "org_id": resp.get("org_id", org_id),
                "response": resp,
                "error": None,
            }
        except requests.HTTPError as e:
            body = getattr(e.response, "text", "")[:2000]
            row = {
                "client_id": client_id_str,
                "controlset_id": controlset_id,
                "job_status": None,
                "status": None,
                "compliance_name": None,
                "created_by": None,
                "org_id": org_id,
                "response": None,
                "error": f"HTTP {e.response.status_code}: {body}",
            }
        except Exception as e:
            row = {
                "client_id": client_id_str,
                "controlset_id": controlset_id,
                "job_status": None,
                "status": None,
                "compliance_name": None,
                "created_by": None,
                "org_id": org_id,
                "response": None,
                "error": repr(e),
            }

        results.append(row)
        if per_request_sleep_sec:
            time.sleep(per_request_sleep_sec)

    # 3) Persist and return a summary
    df = pd.json_normalize(results, sep=".")
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    os.makedirs("audit_results", exist_ok=True)

    jsonl_path = f"audit_results/audit_jobs_{ts}.jsonl"
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    csv_path = f"audit_results/audit_jobs_{ts}.csv"
    df.to_csv(csv_path, index=False)

    print(f"Saved JSONL → {jsonl_path}")
    print(f"Saved CSV   → {csv_path}")
    return df


# Variabes
BASE_URL_BE = "YOUR_URL"
USERNAME = "YOUR USERNAME"
PASSWORD = "YOUR PASSWORD"

# Evidence Dictionary
EVIDENCE_MAP  = evidence_mapping

# Get Token
token = be_login(USERNAME, PASSWORD)
print("Logged in to BE. Token acquired.")

# Running the batch audits
df_summary = run_batch_audits_with_map(
    token,
    EVIDENCE_MAP,
    companies_requiring_audit,           # <-- REQUIRED DataFrame input
    iq_id_col="IQ ID",
    controlset_col="Control_Set_ID",
    compliance_name_prefix="Capstone Audit",
    description="v1",
    org_id="GMU_Capstone",
    per_request_sleep_sec=0.5,
)

# Example Use
#print("\nSummary:")
#display(df_summary)
